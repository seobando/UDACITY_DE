# UDACITY NANODEGREE - DATA ENGINEER

[Udacity](https://www.udacity.com/) I think, is the best online tech learning platform in the world, that offers a series of courses in the digital industry and also something called Nanodegree, like their own kind of degree, focused on the knowledge required by a specific role, in this case, a Data Engineer.

To accomplish the nanodegree requirements you have to develop a series of projects and a final capstones project, the projects are defined by udacity, and the capstones project born from your own ideas.

The stack of technologies learned in the nanodegree program are:

1. [PostgresSQL](https://www.postgresql.org/): This is a SQL database technology, used to develop relational databases. 
2. [Cassandra](https://cassandra.apache.org/): This is a NoSQL (Not only SQL) database technology.
3. [Amazon Redshift](https://aws.amazon.com/es/redshift/?whats-new-cards.sort-by=item.additionalFields.postDateTime&whats-new-cards.sort-order=desc): This is the Data Warehouse technology of AWS Cloud.
4. [Spark](http://spark.apache.org/): is the Data Engineer tool for excellency, is a powerful solution to deal with Big Data in a fast and efficient way.
5. [Airflow](https://airflow.apache.org/): is another of the most important tools in the Data Engineer tool kit, it enables the orchestration of different data components in a data-based solution.

In total there are 5 projects:

1. [Data Modelling with PostgresSQL](https://github.com/seobando/UDACITY_DE/tree/master/1_data_modelling_postgresSQL)  
2. [Data Modelling with Apache Cassandra](https://github.com/seobando/UDACITY_DE/tree/master/2_data_modelling_cassandra)
3. [Data Warehouse building with Amazon Redshift](https://github.com/seobando/UDACITY_DE/tree/master/3_data_warehouse)
4. [Data Lake building with Spark](https://github.com/seobando/UDACITY_DE/tree/master/4_data_lake)
5. [Data Pipelines building with Airflow](https://github.com/seobando/UDACITY_DE/tree/master/5_pipeline)

The datasets used for all the project are almost the same and the output requirements for this data is also almost the same, just developing ETLs for moving and transforming data. The main objective of the projects is to get used to working with these technologies in terms of what data engineers do in their daily work.

### DISCLAIMER

- The pipeline I'm working in the pipeline project and when I finished I'll start with the capstone, so if the code for the project 1-4 are working code.

