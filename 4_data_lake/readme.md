# PROJECT - DATA LAKE

The technologies learning in this project were Spark developing ETLs and S3 for storage as a Data Lake.

## PURPOSE

This is the fourth udacity nanodegree project, which consists of developing a datalake for a music streaming startup called Sparkify. The objective is to take the input data sets and through a series of ETLs convert them into a fact table and a series of dimensional tables to obtain the star shape schema required to implement the analytics required by the Sparkify team. 

## DATASETS

Data sets specify their name, description, and fields, as shown below:

- Input Data:

    - song_data - subset of real data from the [Million Song Dataset](http://millionsongdataset.com/): artist_id, artist_latitude,artist_location,artist_longitude,artist_name,duration,num_songs,song_id, title, year

    - log_data -  log files in JSON format generated by this [event simulator](https://github.com/Interana/eventsim) based on the songs in the dataset above.: artist, auth,first_name,gender,item_in_session,last_name,length,level,location,method,page,registration,session_id,song,status,ts,user_agent,user_id    

- Output Data:

    - Fact Table
    
        - songplays - records in log data associated with song plays i.e. records with page NextSong: songplay_id, start_time, user_id, level, song_id, artist_id, session_id, location, user_agent

    - Dimension Tables

        - users - users in the app: user_id, first_name, last_name, gender, level
        - songs - songs in music database: song_id, title, artist_id, year, duration
        - artists - artists in music database: artist_id, name, location, lattitude, longitude
        - time - timestamps of records in songplays broken down into specific units: start_time, hour, day, week, month, year, weekday

## FILES

The files contained in the following repository are:

- etl.py: reads data from S3, processes that data using Spark, and writes them back to S3
- etl.ipynb: is the etl version in a notebook for local use
- dl.cfg: contains your AWS credentials
- README.md: provides discussion on your process and decisions
- data: contains the compressed datasets son_data and log_data
- input: contains the uncompressed data sets
- output: contains the tables of the star schema required by Sparkify

## FOLLOW NEXT INSTRUCTION TO RUN THE PROJECT

It is assumed that you have knowledge of Spark and you have Spark installed and correctly configured, for this exercise Spark version 2.4.3 was used, that said the steps to deploy the project are:

If S3 is to be used as data lake:

0. Download and unzip the project on a local machine
1. Change the required fields to yours in the dl.cfg file to enable access to AWS
2. Create a bucket in S3 with the name udacity-dend
3. Upload the input folder
4. Create an output folder
5. Change the directories for input and output in main according to the input and output directory of your S3
6. Run the etl.py file

If a local machine is to be used as a data lake:

1. Download and unzip the project on a local machine
2. Run the etl.ipynb file
